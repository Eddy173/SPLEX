{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56998ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Prediction RMSE (with sign-transfer): 1.4020472557052739\n",
      "Quality Assessment Accuracy (with sign-transfer): 0.55\n"
     ]
    }
   ],
   "source": [
    "# optimal_pipeline.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import (roc_auc_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix, balanced_accuracy_score, average_precision_score)\n",
    "\n",
    "\n",
    "def testModel(y_true, y_pred, y_prob):    \n",
    "    #Confusion Matrix\n",
    "    cm=confusion_matrix(y_true, y_pred)\n",
    "    print(\"Confusion Matrix:\\n\", cm) #to understand errors in imbalanced datasets\n",
    "    \n",
    "    #Precision\n",
    "    prec=precision_score(y_true, y_pred)\n",
    "    print(\"Precision (positive class):\", prec) #High precision=few false positives\n",
    "    \n",
    "    #Recall\n",
    "    rec=recall_score(y_true, y_pred)\n",
    "    print(\"Recall (positive class):\", rec) #High recall=few false negatives\n",
    "    \n",
    "    #F1-score\n",
    "    f1=f1_score(y_true, y_pred)\n",
    "    print(\"F1-score (positive class):\", f1) #Harmonic mean of precision & recall\n",
    "    \n",
    "    #Balanced Accuracy\n",
    "    balAcc=balanced_accuracy_score(y_true, y_pred)\n",
    "    print(\"Balanced Accuracy:\", balAcc) #Average of recall for each class\n",
    "    \n",
    "    #ROC AUC\n",
    "    roc=roc_auc_score(y_true, y_prob)\n",
    "    print(\"ROC AUC:\", roc)\n",
    "    \n",
    "    #Average Precision (PR AUC)\n",
    "    ap=average_precision_score(y_true, y_prob)\n",
    "    print(\"Average Precision (PR AUC):\", ap) #Focuses on performance for positive class\n",
    "\n",
    "    \n",
    "# Load data\n",
    "df = pd.read_csv(\"risk_factors_cervical_cancer.csv\")\n",
    "df = df.replace(\"?\", np.nan)\n",
    "df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Drop columns with too many missing values\n",
    "df = df.drop(columns=[\"STDs: Time since first diagnosis\", \"STDs: Time since last diagnosis\"])\n",
    "\n",
    "# Separate binary and continuous features\n",
    "binary_cols = [c for c in df.columns if df[c].dropna().isin([0,1]).all()]\n",
    "continuous_cols = list(set(df.columns) - set(binary_cols) - {\"Biopsy\"})\n",
    "\n",
    "# Impute missing values\n",
    "binary_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "df[binary_cols] = binary_imputer.fit_transform(df[binary_cols])\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df[continuous_cols] = knn_imputer.fit_transform(df[continuous_cols])\n",
    "\n",
    "# Define features and target\n",
    "X=df.drop(\"Biopsy\", axis=1)\n",
    "y=df[\"Biopsy\"]\n",
    "\n",
    "# Feature selection: top 10 features via ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "print(\"Class distribution after SMOTE:\", pd.Series(y_train_res).value_counts())\n",
    "\n",
    "# Model 1: Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "lr.fit(X_train_res, y_train_res)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "y_prob_lr = lr.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "testModel(y_test, y_pred_lr, y_prob_lr)\n",
    "\n",
    "\n",
    "# Model 2: Gradient Boosting\n",
    "gb = GradientBoostingClassifier(n_estimators=300, random_state=42)\n",
    "gb.fit(X_train_res, y_train_res)\n",
    "y_pred_gb = gb.predict(X_test_scaled)\n",
    "y_prob_gb = gb.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "print(\"\\nGradient Boosting Results:\")\n",
    "testModel(y_test, y_pred_gb, y_prob_gb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a79fc90",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (33,) (32,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m Xt_test_scaled \u001b[38;5;241m=\u001b[39m scaler_tgt\u001b[38;5;241m.\u001b[39mtransform(Xt_test)\n\u001b[0;32m     81\u001b[0m sign_lr \u001b[38;5;241m=\u001b[39m SignTransferLogisticRegression(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m sign_lr\u001b[38;5;241m.\u001b[39mfit(Xt_train_scaled, yt_train, sign_source)\n\u001b[0;32m     84\u001b[0m yt_pred_sign \u001b[38;5;241m=\u001b[39m sign_lr\u001b[38;5;241m.\u001b[39mpredict(Xt_test_scaled)\n\u001b[0;32m     85\u001b[0m yt_prob_sign \u001b[38;5;241m=\u001b[39m sign_lr\u001b[38;5;241m.\u001b[39mpredict_proba(Xt_test_scaled)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m, in \u001b[0;36mSignTransferLogisticRegression.fit\u001b[1;34m(self, X, y, sign_src)\u001b[0m\n\u001b[0;32m     25\u001b[0m y_signed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m w0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 28\u001b[0m res \u001b[38;5;241m=\u001b[39m minimize(\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss,\n\u001b[0;32m     30\u001b[0m     w0,\n\u001b[0;32m     31\u001b[0m     args\u001b[38;5;241m=\u001b[39m(X, y_signed, sign_src),\n\u001b[0;32m     32\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m     options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter}\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mx\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bima\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    714\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bima\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:347\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    344\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# _prepare_scalar_function can use bounds=None to represent no bounds\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m sf \u001b[38;5;241m=\u001b[39m _prepare_scalar_function(fun, x0, jac\u001b[38;5;241m=\u001b[39mjac, args\u001b[38;5;241m=\u001b[39margs, epsilon\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    348\u001b[0m                               bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[0;32m    349\u001b[0m                               finite_diff_rel_step\u001b[38;5;241m=\u001b[39mfinite_diff_rel_step)\n\u001b[0;32m    351\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[0;32m    353\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bima\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:288\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    284\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m sf \u001b[38;5;241m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0;32m    289\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bima\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:166\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bima\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl()\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bima\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\bima\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mSignTransferLogisticRegression._loss\u001b[1;34m(self, w, X, y, sign_src)\u001b[0m\n\u001b[0;32m     17\u001b[0m log_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39my \u001b[38;5;241m*\u001b[39m z)))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Sign disagreement penalty (hinge loss)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m sign_penalty \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39mw \u001b[38;5;241m*\u001b[39m sign_src))\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m sign_penalty\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (33,) (32,) "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73b0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
